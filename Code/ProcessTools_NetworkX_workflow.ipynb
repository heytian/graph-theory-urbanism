{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This python model helps with computation of key metrics such as NQPD, needed for multifunctionality calculations."
      ],
      "metadata": {
        "id": "j-aOqijkDhMe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IjZW_sP_fid",
        "outputId": "1f4133ac-d89d-4bbb-83d1-da141d78c884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_path = '/content/drive/MyDrive/Boston_data_colab'\n"
      ],
      "metadata": {
        "id": "-Cq2SJ9rAqzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "from collections import defaultdict\n",
        "import geopandas as gpd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import shapely\n",
        "from shapely.geometry import LineString"
      ],
      "metadata": {
        "id": "5-ex_bBJAsDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Street Network Pre-processing + Angular bearing calculations**"
      ],
      "metadata": {
        "id": "idTTDh1GDnIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streets = gpd.read_file('/content/drive/MyDrive/Boston_data_colab/boston_streets_good/boston_streets_good.shp')\n",
        "\n",
        "\n",
        "num_segments = len(streets)\n",
        "print(f\"Number of segments: {num_segments}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_W620PsqJuG",
        "outputId": "acb0da78-ece9-4f27-fbb3-9417758ae580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of segments: 18328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_streets(streets):\n",
        "  \"Streets data preprocessing\"\n",
        "\n",
        "    def drop_z_coord(geom):\n",
        "        coords_2d = [(x,y) for x, y, *rest in geom.coords]\n",
        "        return LineString(coords_2d)\n",
        "\n",
        "    streets[\"geometry\"] = streets.geometry.apply(drop_z_coord)\n",
        "    streets = streets.to_crs(epsg=32619)\n",
        "\n",
        "    def get_endpoints(geom):\n",
        "        coords = list(geom.coords)\n",
        "        return (coords[0], coords[-1])\n",
        "\n",
        "    streets[\"endpoints\"] = streets.geometry.apply(get_endpoints)\n",
        "    streets[\"start_point\"] = streets[\"endpoints\"].apply(lambda x: x[0])\n",
        "    streets[\"end_point\"] = streets[\"endpoints\"].apply(lambda x: x[1])\n",
        "    streets[\"length\"] = streets.geometry.length\n",
        "\n",
        "\n",
        "    return streets"
      ],
      "metadata": {
        "id": "NSI9NXnYA1bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_angular_bearing(line):\n",
        "    start, end = line.coords[0], line.coords[-1]\n",
        "    dx, dy = end[0] - start[0],  end[1] - start[1]\n",
        "    angle = math.degrees(math.atan2(dy, dx)) % 360\n",
        "    return angle"
      ],
      "metadata": {
        "id": "js-9NjCZET-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Function For Optimized dual graph construction**"
      ],
      "metadata": {
        "id": "B-YVynbdELFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_optimized_graph(streets):\n",
        "    \"\"\"Building graph with adjacency lists for faster neighbor access\"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add segments as nodes WITH length attribute\n",
        "    for sid in streets.index:\n",
        "        G.add_node(sid, length=streets.loc[sid, 'length'])\n",
        "\n",
        "    # Build adjacency structure for O(1) neighbor lookup\n",
        "    endpts_to_segments = defaultdict(set)\n",
        "    for i, row in streets.iterrows():\n",
        "        endpts_to_segments[row[\"start_point\"]].add(i)\n",
        "        endpts_to_segments[row[\"end_point\"]].add(i)\n",
        "\n",
        "    def angular_cost(b1, b2):\n",
        "        diff = abs(b1 - b2)\n",
        "        return min(diff, 360 - diff)\n",
        "\n",
        "    # Pre-compute edge weights and store in efficient structure\n",
        "    edge_weights = {}\n",
        "    for segment_group in endpts_to_segments.values():\n",
        "        segment_list = list(segment_group)\n",
        "        for i in range(len(segment_list)):\n",
        "            for j in range(i + 1, len(segment_list)):\n",
        "                node1, node2 = segment_list[i], segment_list[j]\n",
        "                a1 = streets.iloc[node1][\"bearing\"]\n",
        "                a2 = streets.iloc[node2][\"bearing\"]\n",
        "                weight = angular_cost(a1, a2)\n",
        "                G.add_edge(node1, node2, weight=weight)\n",
        "                # Store for quick lookup\n",
        "                edge_weights[(node1, node2)] = weight\n",
        "                edge_weights[(node2, node1)] = weight\n",
        "\n",
        "    return G, edge_weights"
      ],
      "metadata": {
        "id": "Y-PgHMkiEZWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Function that helps calculate both angular distances and Euclidean path lengths from a source node to all other reachable nodes with a specified radius using Dijkstra's algorithm**"
      ],
      "metadata": {
        "id": "WWCr1jamEZyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def heap_dijkstra_radius_constrained(G, source, radius_meters, node_lengths):\n",
        "    \"\"\"\n",
        "    HEAP-OPTIMIZED Dijkstra's algorithm with radius constraint.\n",
        "    Returns both angular distances and euclidean path lengths.\n",
        "    \"\"\"\n",
        "    # Initialize distances and path lengths\n",
        "    angular_distances = {source: 0}\n",
        "    path_lengths = {source: 0}\n",
        "    visited = set()\n",
        "\n",
        "    # Min-heap: (angular_distance, node, euclidean_length)\n",
        "    heap = [(0, source, 0)]\n",
        "\n",
        "    while heap:\n",
        "        current_angular_dist, current_node, current_length = heapq.heappop(heap)\n",
        "\n",
        "        # Skip if already visited or exceeds radius\n",
        "        if current_node in visited or current_length > radius_meters:\n",
        "            continue\n",
        "\n",
        "        visited.add(current_node)\n",
        "\n",
        "        # Process all neighbors\n",
        "        for neighbor in G.neighbors(current_node):\n",
        "            if neighbor in visited:\n",
        "                continue\n",
        "\n",
        "            # Get edge weight and neighbor length\n",
        "            edge_weight = G[current_node][neighbor]['weight']\n",
        "            neighbor_length = node_lengths[neighbor]\n",
        "\n",
        "            # Calculate new distances\n",
        "            new_angular_dist = current_angular_dist + edge_weight\n",
        "            new_length = current_length + neighbor_length\n",
        "\n",
        "            # Check radius constraint\n",
        "            if new_length > radius_meters:\n",
        "                continue\n",
        "\n",
        "            # Update if we found a better path\n",
        "            if (neighbor not in angular_distances or\n",
        "                new_angular_dist < angular_distances[neighbor]):\n",
        "\n",
        "                angular_distances[neighbor] = new_angular_dist\n",
        "                path_lengths[neighbor] = new_length\n",
        "                heapq.heappush(heap, (new_angular_dist, neighbor, new_length))\n",
        "\n",
        "    return angular_distances, path_lengths"
      ],
      "metadata": {
        "id": "9874nPDzEaA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Defining a function that calculates POI densities on each street segment. This will be helpful for gravity centrality calculations**"
      ],
      "metadata": {
        "id": "45t76qjkEoAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_poi_densities_on_segments(streets, poi_gdfs_dict, buffer_distance=50):\n",
        "    \"\"\"\n",
        "    Calculate POI densities on each street segment for gravity centrality calculation.\n",
        "    This gives us P(y) - the proportion/density of POIs on each segment.\n",
        "    \"\"\"\n",
        "    print(\"Calculating POI densities on street segments...\")\n",
        "\n",
        "    poi_densities = {}\n",
        "\n",
        "    for poi_category, poi_gdf in poi_gdfs_dict.items():\n",
        "        print(f\"  Processing {poi_category} POI density...\")\n",
        "\n",
        "        # Buffer street segments to capture nearby POIs\n",
        "        streets_buffered = streets.copy()\n",
        "        streets_buffered[\"geometry\"] = streets_buffered.geometry.buffer(buffer_distance)\n",
        "\n",
        "        # Use spatial index for efficiency\n",
        "        poi_sindex = poi_gdf.sindex\n",
        "        segment_poi_counts = {}\n",
        "\n",
        "        for segment_id in streets.index:\n",
        "            segment_geom = streets_buffered.loc[segment_id, 'geometry']\n",
        "\n",
        "            # Find POIs within buffer using spatial index\n",
        "            possible_matches_idx = list(poi_sindex.intersection(segment_geom.bounds))\n",
        "            possible_matches = poi_gdf.iloc[possible_matches_idx]\n",
        "\n",
        "            # Count POIs that actually intersect\n",
        "            intersecting_pois = possible_matches[possible_matches.intersects(segment_geom)]\n",
        "            poi_count = len(intersecting_pois)\n",
        "\n",
        "            # Calculate density as POIs per unit length (or just count)\n",
        "            segment_length = streets.loc[segment_id, 'length']\n",
        "            density = poi_count / segment_length if segment_length > 0 else 0\n",
        "\n",
        "            segment_poi_counts[segment_id] = {\n",
        "                'count': poi_count,\n",
        "                'density': density\n",
        "            }\n",
        "\n",
        "        poi_densities[poi_category] = segment_poi_counts\n",
        "\n",
        "        # Add to streets dataframe for reference\n",
        "        streets[f\"poi_count_{poi_category}\"] = [segment_poi_counts[sid]['count'] for sid in streets.index]\n",
        "        streets[f\"poi_density_{poi_category}\"] = [segment_poi_counts[sid]['density'] for sid in streets.index]\n",
        "\n",
        "    return poi_densities"
      ],
      "metadata": {
        "id": "XJDYpNSmEfjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gravity_centrality_nqpd(G, streets, poi_densities, radius_meters=1000):\n",
        "    \"\"\"\n",
        "    Calculate gravity centrality NQPD for each POI category using the simplified formula:\n",
        "    NQPD(x) = Σ(y∈Rx) W(y) / d(x,y)\n",
        "\n",
        "    Where:\n",
        "    - W(y) = POI density on segment y\n",
        "    - d(x,y) = network distance from segment x to segment y\n",
        "    - Rx = all segments reachable within radius from segment x\n",
        "    \"\"\"\n",
        "    print(f\"Calculating Gravity Centrality NQPD for radius: {radius_meters}m\")\n",
        "\n",
        "    # Pre-compute node lengths for faster access\n",
        "    node_lengths = {node: G.nodes[node]['length'] for node in G.nodes()}\n",
        "\n",
        "    # Results dictionary for each POI category\n",
        "    gravity_nqpd_results = {}\n",
        "\n",
        "    # Process each POI category\n",
        "    for poi_category, poi_density_data in poi_densities.items():\n",
        "        print(f\"  Processing gravity NQPD for {poi_category}...\")\n",
        "\n",
        "        gravity_values = {}\n",
        "        total_segments = len(G.nodes())\n",
        "\n",
        "        for i, source_segment in enumerate(G.nodes()):\n",
        "            if i % 100 == 0:\n",
        "                print(f\"    Processed {i}/{total_segments} segments for {poi_category}\")\n",
        "\n",
        "            # Get all reachable segments within radius using network distance\n",
        "            angular_distances, path_lengths = heap_dijkstra_radius_constrained(\n",
        "                G, source_segment, radius_meters, node_lengths\n",
        "            )\n",
        "\n",
        "            # Calculate gravity centrality sum\n",
        "            gravity_sum = 0.0\n",
        "\n",
        "            for dest_segment, network_distance in path_lengths.items():\n",
        "                if dest_segment == source_segment:\n",
        "                    continue  # Skip self\n",
        "\n",
        "                # W(y) - POI density on destination segment\n",
        "                W_y = poi_density_data[dest_segment]['density']\n",
        "\n",
        "\n",
        "                # P_y = 1\n",
        "\n",
        "                # d(x,y) - network distance (use path length in meters)\n",
        "                d_xy = network_distance\n",
        "\n",
        "                # Avoid division by zero\n",
        "                if d_xy > 0 and W_y > 0:\n",
        "                    # Add to gravity sum: W(y) / d(x,y) (P ignored)\n",
        "                    gravity_contribution = W_y / d_xy\n",
        "                    gravity_sum += gravity_contribution\n",
        "\n",
        "            gravity_values[source_segment] = gravity_sum\n",
        "\n",
        "        gravity_nqpd_results[poi_category] = gravity_values\n",
        "\n",
        "        # Print summary statistics\n",
        "        values = list(gravity_values.values())\n",
        "        if values:\n",
        "            print(f\"    {poi_category} gravity NQPD: mean={np.mean(values):.6f}, max={np.max(values):.6f}\")\n",
        "\n",
        "    return gravity_nqpd_results"
      ],
      "metadata": {
        "id": "R41XQ3zvEkQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Carrying out spatial joins with street segments and population polygons data**"
      ],
      "metadata": {
        "id": "bv7fehMYE0HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def heap_optimized_spatial_joins(streets, population_gdf, poi_gdfs_dict):\n",
        "    \"\"\"\n",
        "    Optimized spatial joins using spatial indexing (original function)\n",
        "    \"\"\"\n",
        "    print(\"Performing HEAP-OPTIMIZED spatial joins...\")\n",
        "\n",
        "    # Step 1: Network -> Population (30m) with spatial index\n",
        "    print(\"Step 1: Network -> Population (30m radius) - OPTIMIZED\")\n",
        "    streets_pop = streets.copy()\n",
        "    streets_pop[\"geometry\"] = streets_pop.geometry.buffer(30)\n",
        "\n",
        "    # Use spatial index for faster intersection\n",
        "    pop_sindex = population_gdf.sindex\n",
        "    pop_results = []\n",
        "\n",
        "    for idx, street_geom in streets_pop.geometry.items():\n",
        "        # Use spatial index to find potential matches\n",
        "        possible_matches_idx = list(pop_sindex.intersection(street_geom.bounds))\n",
        "        possible_matches = population_gdf.iloc[possible_matches_idx]\n",
        "\n",
        "        # Precise intersection test\n",
        "        intersects = possible_matches[possible_matches.intersects(street_geom)]\n",
        "        total_pop = intersects[\"POP20\"].sum() if len(intersects) > 0 else 0\n",
        "        pop_results.append(total_pop)\n",
        "\n",
        "    streets[\"POP20\"] = pop_results\n",
        "\n",
        "    # Step 2: Result -> POIs (400m) with spatial indexing\n",
        "    print(\"Step 2: Result -> POIs (400m radius) - OPTIMIZED\")\n",
        "    streets_poi = streets.copy()\n",
        "    streets_poi[\"geometry\"] = streets_poi.geometry.buffer(400)\n",
        "\n",
        "    # Process each POI category with spatial indexing\n",
        "    for category_name, poi_gdf in poi_gdfs_dict.items():\n",
        "        print(f\"  Processing {category_name} with spatial index...\")\n",
        "\n",
        "        poi_sindex = poi_gdf.sindex\n",
        "        poi_counts = []\n",
        "\n",
        "        for idx, street_geom in streets_poi.geometry.items():\n",
        "            # Use spatial index\n",
        "            possible_matches_idx = list(poi_sindex.intersection(street_geom.bounds))\n",
        "            possible_matches = poi_gdf.iloc[possible_matches_idx]\n",
        "\n",
        "            # Precise containment test\n",
        "            contains = possible_matches[possible_matches.within(street_geom)]\n",
        "            count = len(contains)\n",
        "            poi_counts.append(count)\n",
        "\n",
        "        streets[f\"poisdsty_{category_name}\"] = poi_counts\n",
        "\n",
        "    return streets"
      ],
      "metadata": {
        "id": "WyLK0c8yFG9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Overall Function that runs the gravity centrality NQPD analysis for each POI category**"
      ],
      "metadata": {
        "id": "WMYMgrnPFyK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gravity_centrality_boston_analysis():\n",
        "    \"\"\"\n",
        "    Boston analysis with Gravity Centrality NQPD calculations for each POI category.\n",
        "    \"\"\"\n",
        "    print(\"=== GRAVITY CENTRALITY NQPD BOSTON ANALYSIS ===\")\n",
        "\n",
        "    # Load data\n",
        "    streets = gpd.read_file('/content/drive/MyDrive/Boston_data_colab/boston_streets_good/boston_streets_good.shp')\n",
        "\n",
        "    # Preprocess\n",
        "    streets = preprocess_streets(streets)\n",
        "    streets[\"bearing\"] = streets.geometry.apply(compute_angular_bearing)\n",
        "\n",
        "    # Build OPTIMIZED graph\n",
        "    print(\"Building optimized graph structure...\")\n",
        "    G, edge_weights = build_optimized_graph(streets)\n",
        "\n",
        "    # Load POI and population data\n",
        "    poi_gdfs = {\n",
        "        'work': gpd.read_file(\"/content/drive/MyDrive/Boston_data_colab/boston_POIS_work/Boston_POIS_work.shp\").to_crs(epsg=32619),\n",
        "        'educivic': gpd.read_file(\"/content/drive/MyDrive/Boston_data_colab/boston_POIS_educivic/boston_POIS_educivic.shp\").to_crs(epsg=32619),\n",
        "        'leisure': gpd.read_file(\"/content/drive/MyDrive/Boston_data_colab/boston_POIS_leisure/boston_POIS_leisure.shp\").to_crs(epsg=32619),\n",
        "        'shopping': gpd.read_file(\"/content/drive/MyDrive/Boston_data_colab/boston_POIS_shopping/boston_POIS_shopping.shp\").to_crs(epsg=32619)\n",
        "    }\n",
        "\n",
        "    pop = gpd.read_file(\"/content/drive/MyDrive/Boston_data_colab/boston_pop_blocks/boston_pop_blocks.shp\").to_crs(epsg=32619)\n",
        "\n",
        "    # OPTIMIZED spatial joins (original POI density calculations)\n",
        "    streets = heap_optimized_spatial_joins(streets, pop, poi_gdfs)\n",
        "\n",
        "    # Calculate POI densities on segments (P(y) values)\n",
        "    print(\"\\n--- CALCULATING POI DENSITIES ON SEGMENTS ---\")\n",
        "    poi_densities = calculate_poi_densities_on_segments(streets, poi_gdfs, buffer_distance=50)\n",
        "\n",
        "    # Calculate Gravity Centrality NQPD at 1000m radius\n",
        "    print(\"\\n--- CALCULATING GRAVITY CENTRALITY NQPD (1000m) ---\")\n",
        "    gravity_nqpd_results = calculate_gravity_centrality_nqpd(G, streets, poi_densities, radius_meters=1000)\n",
        "\n",
        "    # Add Gravity Centrality NQPD results to streets dataframe\n",
        "    for poi_category, nqpd_values in gravity_nqpd_results.items():\n",
        "        col_name = f\"gravity_nqpd_{poi_category}_1000m\"\n",
        "        streets[col_name] = streets.index.map(nqpd_values).fillna(0)\n",
        "        print(f\"Added column: {col_name}\")\n",
        "\n",
        "    # Calculate multifunctionality as sum of gravity NQPD values\n",
        "    multifunctionality_cols = [f\"gravity_nqpd_{cat}_1000m\" for cat in poi_gdfs.keys()]\n",
        "    streets['multifunctionality_1000m'] = streets[multifunctionality_cols].sum(axis=1)\n",
        "    print(\"Added column: multifunctionality_1000m\")\n",
        "\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    # Export results\n",
        "    print(\"\\nExporting results...\")\n",
        "\n",
        "    # Including only Gravity NQPD columns, multifunctionality, and basic POI data\n",
        "    gravity_cols = [f\"gravity_nqpd_{cat}_1000m\" for cat in poi_gdfs.keys()]\n",
        "    poi_density_cols = [col for col in streets.columns if col.startswith('poi_count_') or col.startswith('poi_density_')]\n",
        "    basic_poi_cols = [col for col in streets.columns if col.startswith('poisdsty_')]\n",
        "    all_output_cols = gravity_cols + poi_density_cols + basic_poi_cols + ['multifunctionality_1000m', 'POP20']\n",
        "\n",
        "    export_df = streets[['geometry'] + all_output_cols]\n",
        "\n",
        "    # Save as text file\n",
        "    text_output = export_df.drop(columns=['geometry'])\n",
        "    text_output.to_csv('/content/drive/MyDrive/Boston_data_colab/boston_gravity_centrality_results.txt', sep='\\t', index=True)\n",
        "\n",
        "    # Save as shapefile\n",
        "    export_df.to_file('/content/drive/MyDrive/Boston_data_colab/boston_gravity_centrality_results.shp')\n",
        "\n",
        "    print(\"Gravity Centrality Analysis complete!\")\n",
        "    print(f\"Results saved to boston_gravity_centrality_results.txt and boston_gravity_centrality_results.shp\")\n",
        "\n",
        "    # Print summary of Gravity Centrality NQPD results\n",
        "    print(\"\\n=== GRAVITY CENTRALITY NQPD SUMMARY ===\")\n",
        "    for poi_category in poi_gdfs.keys():\n",
        "        col_name = f\"gravity_nqpd_{poi_category}_1000m\"\n",
        "        if col_name in streets.columns:\n",
        "            mean_val = streets[col_name].mean()\n",
        "            max_val = streets[col_name].max()\n",
        "            std_val = streets[col_name].std()\n",
        "            print(f\"{poi_category}: mean={mean_val:.6f}, max={max_val:.6f}, std={std_val:.6f}\")\n",
        "\n",
        "    # Multifunctionality summary\n",
        "    print(f\"\\nMultifunctionality: mean={streets['multifunctionality_1000m'].mean():.6f}, max={streets['multifunctionality_1000m'].max():.6f}\")\n",
        "\n",
        "    return streets, all_results, gravity_nqpd_results\n"
      ],
      "metadata": {
        "id": "vnyhS_ezFQpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the gravity centrality analysis\n",
        "if __name__ == \"__main__\":\n",
        "    streets_results, analysis_results, gravity_nqpd_results = run_gravity_centrality_boston_analysis()"
      ],
      "metadata": {
        "id": "Aq5yrlY6GAwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Exporting Result as shp file for GIS**"
      ],
      "metadata": {
        "id": "XMbpoUZdGIWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "\n",
        "# === Extract endpoints into x1, y1, x2, y2 ===\n",
        "streets_results[['x1', 'y1']] = streets_results['endpoints'].apply(lambda ep: pd.Series(ep[0]))\n",
        "streets_results[['x2', 'y2']] = streets_results['endpoints'].apply(lambda ep: pd.Series(ep[1]))\n",
        "\n",
        "# === Rename long NQPD columns to shapefile-safe names ===\n",
        "rename_map = {\n",
        "    'gravity_nqpd_work_1000m': 'nqpd_work',\n",
        "    'gravity_nqpd_educivic_1000m': 'nqpd_edu',\n",
        "    'gravity_nqpd_shopping_1000m': 'nqpd_shop',\n",
        "    'gravity_nqpd_leisure_1000m': 'nqpd_leis',\n",
        "    'multifunctionality_1000m': 'nqpd_total',\n",
        "    'POP20': 'pop20'\n",
        "}\n",
        "streets_results = streets_results.rename(columns=rename_map)\n",
        "\n",
        "# === Define export columns (must include geometry) ===\n",
        "columns_to_export = [\n",
        "    'x1', 'y1', 'x2', 'y2',\n",
        "    'pop20',\n",
        "    'nqpd_work', 'nqpd_edu', 'nqpd_shop', 'nqpd_leis', 'nqpd_total',\n",
        "    'geometry'\n",
        "]\n",
        "\n",
        "# === Create shapefile GeoDataFrame ===\n",
        "export_shp = streets_results[columns_to_export].copy()\n",
        "\n",
        "# === Optional: round for clean output ===\n",
        "numeric_cols = [col for col in export_shp.columns if col != 'geometry']\n",
        "export_shp[numeric_cols] = export_shp[numeric_cols].round(6)\n",
        "\n",
        "# === Export shapefile ===\n",
        "export_shp.to_file(\n",
        "    '/content/drive/MyDrive/Boston_data_colab/boston_nqpd_shapefile.shp',\n",
        "    driver='ESRI Shapefile'\n",
        ")\n",
        "\n",
        "print(\"Shapefile exported as: boston_nqpd_shapefile.shp (with shortened column names)\")"
      ],
      "metadata": {
        "id": "6H33rf6FvoUO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}