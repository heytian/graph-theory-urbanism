{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6_4BJ2Yzqf8"
   },
   "source": [
    "#### Step 1: fclass processing\n",
    "\n",
    "First prepare POIS (points - the code below is a modular script for fclass processing), streets (lines, processed with simplify, planarize, sDNA prepare network, test it out with sDNA integral analysis to check that its ready), POP (polygons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTS_RsLUzqf9"
   },
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tazu6qtgzqf-"
   },
   "outputs": [],
   "source": [
    "#Combine different fields into one fclass within a shp file - for Leisure.geojson converted to point shp\n",
    "\n",
    "shp_path = r\"D:\\UserData16\\heyutian\\data\\01_Reference\\20250712_Boston_POIS\\original\\Leisure-POINT.shp\"\n",
    "\n",
    "# Define fields to check in order of priority\n",
    "field_priority = [\"amenity\", \"leisure\", \"tourism\"]  # Extend this list as needed\n",
    "output_field = \"fclass\"\n",
    "\n",
    "# --- Add 'fclass' field if it doesn't exist ---\n",
    "existing_fields = [f.name for f in arcpy.ListFields(shp_path)]\n",
    "if output_field not in existing_fields:\n",
    "    arcpy.AddField_management(shp_path, output_field, \"TEXT\")\n",
    "\n",
    "# --- Build the expression ---\n",
    "# Pass all fields into the reclass function\n",
    "expression_fields = \", \".join([f\"!{f}!\" for f in field_priority])\n",
    "expression = f\"reclass({expression_fields})\"\n",
    "\n",
    "# --- Define the code block to check each field in order ---\n",
    "# Accepts variable number of arguments using *args\n",
    "code_block = \"\"\"\n",
    "def reclass(*args):\n",
    "    for val in args:\n",
    "        if val is not None and str(val).strip():\n",
    "            return str(val).lower().replace(' ', '_')\n",
    "    return \"unknown\"\n",
    "\"\"\"\n",
    "\n",
    "# --- Calculate field using expression and code block ---\n",
    "arcpy.CalculateField_management(\n",
    "    in_table=shp_path,\n",
    "    field=output_field,\n",
    "    expression=expression,\n",
    "    expression_type=\"PYTHON3\",\n",
    "    code_block=code_block\n",
    ")\n",
    "\n",
    "print(f\"fclass calculated using priority fields: {field_priority}\")\n",
    "\n",
    "# Optional post-processing: Update unknowns based on 'sport'\n",
    "with arcpy.da.UpdateCursor(shp_path, [\"sport\", \"fclass\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        sport_val, fclass_val = row\n",
    "        if fclass_val == \"unknown\" and sport_val is not None and str(sport_val).strip():\n",
    "            row[1] = \"sports_centre\"\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "print(\"Updated fclass to 'sports_centre' where sport exists and fclass was 'unknown'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COX381dGzqf-"
   },
   "outputs": [],
   "source": [
    "# modular script for fclass/merge/clip with library config\n",
    "\n",
    "\n",
    "input_folder = r\"D:\\UserData16\\heyutian\\data\\01_Reference\\20250712_Boston_POIS\\original\"\n",
    "output_folder = r\"D:\\UserData16\\heyutian\\data\\01_Reference\\20250712_Boston_POIS\\processed\"\n",
    "clip_boundary = r\"Z:\\heyutian\\Kaunas_Boston\\Boston\\Boston.gdb\\boston_boundary_edi_Dissolve\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# For each shapefile in input_folder, specify one action:\n",
    "# - \"full\" = add fclass field, calculate it, then clip\n",
    "# - \"clip_only\" = just clip (skip field calc)\n",
    "#   \"merge_only\"  â€“ Do not process or clip, just merge existing file\n",
    "# - \"skip\" = ignore this file completely\n",
    "\n",
    "file_configs = {\n",
    "    \"boston_POIS_work_select.shp\": {\"action\": \"merge_only\"},\n",
    "    \"PLACES_OF_WORSHIP_PT.shp\":     {\"action\": \"full\",\"fixed_fclass\": None,\"source_fields\": [\"TYPE\"]},\n",
    "\n",
    "# example syntax below for various configs:\n",
    "#     \"CHILDCARE_PT.shp\":      {\"action\": \"full\",\"fixed_fclass\": \"childcare\", \"source_fields\": None},\n",
    "#     \"childcare.shp\":     {\"action\": \"full\",\"fixed_fclass\": None,\"source_fields\": [\"CATEGORY\"]},\n",
    "#     \"DCRPOOLS_PT.shp\": {\"action\": \"clip_only\"},\n",
    "#     \"leisure_processed.shp\": {\"action\": \"merge_only\"},  # Already clipped\n",
    "# Add more shapefiles here\n",
    "}\n",
    "\n",
    "# Helper: build CalculateField expression + code block\n",
    "def build_calc_expr_code(fields):\n",
    "    expr = f\"reclass({', '.join(['!' + f + '!' for f in fields])})\"\n",
    "    code_blk = \"\"\"\n",
    "def reclass(*args):\n",
    "    for val in args:\n",
    "        if val is not None and str(val).strip():\n",
    "            return str(val).lower().replace(' ', '_')\n",
    "    return 'unknown'\n",
    "\"\"\"\n",
    "    return expr, code_blk\n",
    "\n",
    "# Collect clipped shapefiles for merging\n",
    "clipped_files = []\n",
    "for fname in os.listdir(input_folder):\n",
    "    if not fname.lower().endswith(\".shp\"):\n",
    "        continue\n",
    "\n",
    "    if fname not in file_configs:\n",
    "        print(f\"Skipping {fname} â€“ not in config\")\n",
    "        continue\n",
    "\n",
    "    config = file_configs[fname]\n",
    "    action = config.get(\"action\", \"skip\")\n",
    "\n",
    "    in_path = os.path.join(input_folder, fname)\n",
    "    base_name = os.path.splitext(fname)[0]\n",
    "    out_path = os.path.join(output_folder, f\"{base_name}_processed.shp\")\n",
    "\n",
    "    if action == \"skip\":\n",
    "        print(f\"Skipping {fname} per config\")\n",
    "        continue\n",
    "\n",
    "    if action == \"merge_only\":\n",
    "        print(f\"Adding {fname} to merge list only (no processing)\")\n",
    "        clipped_files.append(in_path)\n",
    "        continue\n",
    "\n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"Output for {fname} already exists, skipping processing\")\n",
    "        clipped_files.append(out_path)\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {fname} with action '{action}'\")\n",
    "\n",
    "    if action == \"full\":\n",
    "        # Ensure fclass field exists\n",
    "        existing_fields = [f.name for f in arcpy.ListFields(in_path)]\n",
    "        if \"fclass\" not in existing_fields:\n",
    "            arcpy.AddField_management(in_path, \"fclass\", \"TEXT\")\n",
    "\n",
    "        fixed = config.get(\"fixed_fclass\")\n",
    "        source = config.get(\"source_fields\")\n",
    "\n",
    "        if fixed:\n",
    "            with arcpy.da.UpdateCursor(in_path, [\"fclass\"]) as cursor:\n",
    "                for row in cursor:\n",
    "                    row[0] = fixed\n",
    "                    cursor.updateRow(row)\n",
    "            print(f\"Set fixed fclass '{fixed}' for all features\")\n",
    "\n",
    "        elif source:\n",
    "            expr, code_blk = build_calc_expr_code(source)\n",
    "            arcpy.CalculateField_management(in_path, \"fclass\", expr, \"PYTHON3\", code_blk)\n",
    "            print(f\"Calculated fclass from fields: {source}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"No fclass rule specified, skipping classification\")\n",
    "\n",
    "    if action in (\"full\", \"clip_only\"):\n",
    "        arcpy.Clip_analysis(in_path, clip_boundary, out_path)\n",
    "        clipped_files.append(out_path)\n",
    "        print(f\"Clipped to: {out_path}\")\n",
    "\n",
    "# final merge\n",
    "if clipped_files:\n",
    "    merged_output = os.path.join(output_folder, \"boston_POIS_educivic.shp\")\n",
    "    print(f\"Merging {len(clipped_files)} shapefiles into {merged_output}...\")\n",
    "    arcpy.Merge_management(clipped_files, merged_output)\n",
    "    print(\"Merge complete.\")\n",
    "\n",
    "    # --- Deduplication ---\n",
    "    # Count features before\n",
    "    count_before = int(arcpy.GetCount_management(merged_output)[0])\n",
    "\n",
    "    # Delete exact duplicate points with same fclass\n",
    "    arcpy.DeleteIdentical_management(\n",
    "        in_dataset=merged_output,\n",
    "        fields=[\"Shape\",\"fclass\"],              # Consider adding more fields if needed\n",
    "        xy_tolerance=\"0 Meters\",\n",
    "    )\n",
    "\n",
    "    # Count features after\n",
    "    count_after = int(arcpy.GetCount_management(merged_output)[0])\n",
    "    num_deleted = count_before - count_after\n",
    "\n",
    "    print(f\"Removed {num_deleted} duplicate point(s) from merged shapefile.\")\n",
    "    print(f\"Final output saved at: {merged_output}\")\n",
    "\n",
    "else:\n",
    "    print(\"No shapefiles were processed or found to merge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Djttrkjzqf_"
   },
   "outputs": [],
   "source": [
    "# clean up attribute fields that are null or empty (useful for geojsons scraped from overpass turbo)\n",
    "\n",
    "import arcpy\n",
    "\n",
    "shp_path = r\"D:\\UserData16\\heyutian\\data\\01_Reference\\20250712_Boston_POIS\\processed\\boston_POIS_educivic.shp\"\n",
    "\n",
    "# identify fields with null or blank values\n",
    "\n",
    "empty_fields = []\n",
    "\n",
    "# List all fields except OID and geometry\n",
    "fields = [f.name for f in arcpy.ListFields(shp_path) if f.type not in (\"OID\", \"Geometry\")]\n",
    "\n",
    "for field in fields:\n",
    "    has_data = False\n",
    "    with arcpy.da.SearchCursor(shp_path, [field]) as cursor:\n",
    "        for row in cursor:\n",
    "            value = row[0]\n",
    "            if value not in [None, \"\", \" \"]:\n",
    "                has_data = True\n",
    "                break\n",
    "    if not has_data:\n",
    "        empty_fields.append(field)\n",
    "\n",
    "print(\"Fields that are empty across all rows:\")\n",
    "for f in empty_fields:\n",
    "    print(\"   -\", f)\n",
    "\n",
    "# delete those empty rows\n",
    "if empty_fields:\n",
    "    arcpy.DeleteField_management(shp_path, empty_fields)\n",
    "    print(f\"ðŸ§¹ Deleted {len(empty_fields)} empty fields.\")\n",
    "else:\n",
    "    print(\"No empty fields to delete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwFBJxqrzqf_"
   },
   "source": [
    "#### Step 2: Spatial joins - road with POIS\n",
    "Spatial join road network with each POIS (cumulatively), within a distance, 400m. Create new column, join count = pois_shop (this is the POIS density). Then spatial join this resulting file with point data files POIS work, POIS educivic, etc. end with 1 file with 4 pois columns\n",
    "Kaunas_streets_good with Kaunas_POIS_work, then the result with Kaunas_POIS_leisure, etc\n",
    "End with 1 file: Kaunas_st_allPOIS, with 4 columns of 4 POIS densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSadMb2ozqf_",
    "outputId": "e01f25d0-64f0-4a93-c3bb-832c40066986",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining boston_streets_good.shp with boston_POIS_educivic.shp â†’ boston_streets_good_educivic\n",
      "Saved: Z:\\heyutian\\Kaunas_Boston\\Boston\\Boston.gdb\\boston_streets_good_educivic\n",
      "Calculated density field: p_educivic from Join_Count\n",
      "Deleted intermediate Join_Count field: Join_Count\n",
      "Joining boston_streets_good_educivic with boston_POIS_leisure.shp â†’ boston_streets_good_educivic_leisure\n",
      "Saved: Z:\\heyutian\\Kaunas_Boston\\Boston\\Boston.gdb\\boston_streets_good_educivic_leisure\n",
      "Calculated density field: p_leisure from Join_Count\n",
      "Deleted intermediate Join_Count field: Join_Count\n",
      "Joining boston_streets_good_educivic_leisure with boston_POIS_shopping.shp â†’ boston_streets_good_educivic_leisure_shopping\n",
      "Saved: Z:\\heyutian\\Kaunas_Boston\\Boston\\Boston.gdb\\boston_streets_good_educivic_leisure_shopping\n",
      "Calculated density field: p_shopping from Join_Count\n",
      "Deleted intermediate Join_Count field: Join_Count\n",
      "Joining boston_streets_good_educivic_leisure_shopping with boston_POIS_work.shp â†’ boston_streets_good_educivic_leisure_shopping_work\n",
      "Saved: Z:\\heyutian\\Kaunas_Boston\\Boston\\Boston.gdb\\boston_streets_good_educivic_leisure_shopping_work\n",
      "Calculated density field: p_work from Join_Count\n",
      "Deleted intermediate Join_Count field: Join_Count\n",
      "ðŸ§¹ Final cleanup: Deleted fields â†’ ['TARGET_FID', 'TARGET_FID_1', 'TARGET_FID_12', 'TARGET_FID_12_13', 'Join_Count_1', 'TARGET_FID_12_13_14', 'RDTYPE', 'Shape_Leng', 'Part_Count']\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "# define file paths\n",
    "input_folder = r\"D:\\UserData16\\heyutian\\data\\01_Reference\\20250730_Boston_spatialjoins\\input\"\n",
    "output_folder = r\"D:\\UserData16\\heyutian\\data\\01_Reference\\20250730_Boston_spatialjoins\\output\"\n",
    "\n",
    "# find files\n",
    "streets = [f for f in os.listdir(input_folder) if f.startswith(\"boston_streets\") and f.endswith(\".shp\")]\n",
    "POIS = [f for f in os.listdir(input_folder) if f.startswith(\"boston_POIS\") and f.endswith(\".shp\")]\n",
    "\n",
    "# Use the first matching streets file\n",
    "base_streets_file = streets[0]\n",
    "current_streets_path = os.path.join(input_folder, base_streets_file)\n",
    "\n",
    "pois_density_fields = []\n",
    "\n",
    "def build_nullable_fieldmap(target_fc, join_fc):\n",
    "    field_mappings = arcpy.FieldMappings()\n",
    "    # Add all fields from the streets (target) layer only\n",
    "    field_mappings.addTable(target_fc)\n",
    "    # Do NOT add any fields from join_fc (POIS)\n",
    "    # Join_Count field will be auto-generated by SpatialJoin\n",
    "    return field_mappings\n",
    "\n",
    "# Track cumulative suffix for naming\n",
    "cumulative_suffix = \"\"\n",
    "\n",
    "# Loop through POI files and join each cumulatively\n",
    "for i, poi_file in enumerate(POIS):\n",
    "    poi_path = os.path.join(input_folder, poi_file)\n",
    "    poi_suffix = poi_file.split(\"boston_POIS_\")[-1].replace(\".shp\", \"\")\n",
    "    cumulative_suffix += f\"_{poi_suffix}\"\n",
    "\n",
    "    gdb_path = r\"Z:\\heyutian\\Kaunas_Boston\\Boston\\Boston.gdb\"\n",
    "    out_name = f\"{os.path.splitext(base_streets_file)[0]}{cumulative_suffix}\"\n",
    "    out_path = os.path.join(gdb_path, out_name)\n",
    "\n",
    "\n",
    "    print(f\"Joining {os.path.basename(current_streets_path)} with {poi_file} â†’ {out_name}\")\n",
    "\n",
    "    # Prepare field map\n",
    "    field_map = build_nullable_fieldmap(current_streets_path, poi_path)\n",
    "\n",
    "    # Perform spatial join\n",
    "    arcpy.analysis.SpatialJoin(\n",
    "        target_features=current_streets_path,\n",
    "        join_features=poi_path,\n",
    "        out_feature_class=out_path,\n",
    "        join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "        join_type=\"KEEP_ALL\",\n",
    "        field_mapping=field_map,\n",
    "        match_option=\"WITHIN_A_DISTANCE\",\n",
    "        search_radius=\"400 Meters\"\n",
    "    )\n",
    "\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "    # Add POIS density field\n",
    "    # Truncate field name to max 10 chars (safe for shapefiles)\n",
    "    density_field = (f\"p_{poi_suffix.lower()}\")[:10]\n",
    "    # Add field with truncated name, but human-readable alias\n",
    "    arcpy.AddField_management(out_path, density_field, \"DOUBLE\", field_alias=f\"POIS_{poi_suffix}\")\n",
    "\n",
    "    pois_density_fields.append(density_field)\n",
    "\n",
    "\n",
    "    # Determine the join count field name\n",
    "    fields = [f.name for f in arcpy.ListFields(out_path)]\n",
    "    join_count_field = next((f for f in fields if f.lower().startswith(\"join_cou\")), None)\n",
    "\n",
    "    if join_count_field:\n",
    "        arcpy.CalculateField_management(out_path, density_field, f\"!{join_count_field}!\", \"PYTHON3\")\n",
    "        print(f\"Calculated density field: {density_field} from {join_count_field}\")\n",
    "        # Delete the Join_Count field right after\n",
    "        arcpy.DeleteField_management(out_path, join_count_field)\n",
    "        print(f\"Deleted intermediate Join_Count field: {join_count_field}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: Join_Count field not found!\")\n",
    "\n",
    "\n",
    "    # Update current path to new output for next cumulative join\n",
    "    current_streets_path = out_path\n",
    "\n",
    "# After all joins are done\n",
    "all_fields = [f.name for f in arcpy.ListFields(current_streets_path)]\n",
    "\n",
    "# Fields to keep: Shape + fclass + POIS Density Fields\n",
    "keep_fields = ['OBJECTID', 'Shape', 'Shape_Length', 'Shape_Area','fclass'] + pois_density_fields\n",
    "\n",
    "# Determine fields to delete\n",
    "delete_fields = [f for f in all_fields if f not in keep_fields and f.lower() != 'shape']\n",
    "\n",
    "# Delete them in bulk\n",
    "if delete_fields:\n",
    "    arcpy.DeleteField_management(current_streets_path, delete_fields)\n",
    "    print(f\"ðŸ§¹ Final cleanup: Deleted fields â†’ {delete_fields}\")\n",
    "else:\n",
    "    print(\"No extra fields to delete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlOfjKtfzqf_"
   },
   "source": [
    "#### Step 3: sDNA analysis\n",
    "(new step) run SDNA analysis with input polyline features from the previous step i.e. with all 4 POIS densities, but weighted by 4 different POIS\n",
    "e.g. 1k x 4 POIS. Unlike the previous step which requires the output to be inside gdb, this step for sDNA toolbox to work the input polyline file has to be outside of a gdb.\n",
    "\"Integral Analysis\" -> check \"betweenness is bidirectional\", radius = 1000; check \"continuous space\", run\n",
    "Destination weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBQd3oibzqgA",
    "outputId": "63db438d-ebf7-47c1-d76d-8f1825019298"
   },
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "input_polyline = r\"D:\\UserData16\\heyutian\\data\\01_Reference\\20250804_sDNA_POIS_Boston_st_gd\\boston_st_gd_AllPOIS.shp\"\n",
    "output_folder = r\"D:\\UserData16\\heyutian\\data\\01_Reference\\20250804_sDNA_POIS_Boston_st_gd\"\n",
    "\n",
    "# List of your POIS density fields (adjust names exactly as in attribute table)\n",
    "pois_density_fields = ['p_educivic', 'p_leisure', 'p_shopping', 'p_work']  # example names\n",
    "\n",
    "# Import sDNA toolbox with alias 'sdna'\n",
    "sdna_toolbox_path = r\"Z:\\heyutian\\sDNA\\sDNA.pyt\"\n",
    "arcpy.ImportToolbox(sdna_toolbox_path, \"sdna\")\n",
    "\n",
    "# Ensure output folder exists\n",
    "if not arcpy.Exists(output_folder):\n",
    "    arcpy.CreateFolder_management(os.path.dirname(output_folder), os.path.basename(output_folder))\n",
    "\n",
    "# Create a Feature Layer from the input polyline feature class\n",
    "input_layer = \"input_streets_layer\"\n",
    "arcpy.MakeFeatureLayer_management(input_polyline, input_layer)\n",
    "\n",
    "for weight_field in pois_density_fields:\n",
    "    print(f\"Running sDNA Integral analysis weighted by {weight_field}...\")\n",
    "\n",
    "    # Define output path\n",
    "    out_name = f\"boston_sDNA1k_{weight_field}\"\n",
    "    out_path = os.path.join(output_folder, out_name)\n",
    "\n",
    "    # Run the Integral Analysis tool with your parameters\n",
    "    arcpy.sdna.sDNAIntegral(\n",
    "    input=input_layer,\n",
    "    output=out_path,\n",
    "    betweenness=True,\n",
    "    bidir=True,\n",
    "    junctions=False,\n",
    "    hull=False,\n",
    "    start_gs=None,\n",
    "    end_gs=None,\n",
    "    analmet=\"EUCLIDEAN\",\n",
    "    radii=\"1000\",\n",
    "    bandedradii=False,\n",
    "    cont=True,\n",
    "    radmet=\"EUCLIDEAN\",\n",
    "    weighting=\"Link\",\n",
    "    origweight=None,\n",
    "    destweight=weight_field,\n",
    "    zonefiles=None,\n",
    "    odfile=None,\n",
    "    custommetric=None,\n",
    "    disable=\"\",\n",
    "    oneway=None,\n",
    "    intermediates=\"\",\n",
    "    advanced=\"\"\n",
    "    )\n",
    "\n",
    "    print(f\"Saved sDNA Integral output to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_UUW_z1zqgA"
   },
   "source": [
    "#### Step 4: Spatial joins - NQPD combination then with population\n",
    "Rename the NQPD column to each POI, e.g. nqpd1kwork, before each join, so that they remain discrete. Spatial join the 4 sDNA_POIS_work etc files cumulatively, such that there are 4 columns. Keep default spatial join settings (i.e. intersect, no meters defined).\n",
    "Spatial join network with POP with 30m radius\n",
    "End result will be 5 columns: POP, nqpd1kwork, nqpd1klsr, nqpd1kshop, nqpd1kedcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BzxyYXnzqgA",
    "outputId": "a538456f-e826-498f-b81c-7f406a20dc57"
   },
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "input_shp_folder = r\"D:\\UserData16\\heyutian\\data\\01_Reference\\20250804_sDNA_POIS_Boston_st_gd\"\n",
    "gdb_path = r\"Z:\\heyutian\\Kaunas_Boston\\Boston\\Boston.gdb\"\n",
    "pop_layer = r\"D:\\UserData16\\heyutian\\data\\00_Clean\\Boston_data\\boston_pop_blocks.shp\"\n",
    "output_final = r\"D:\\UserData16\\heyutian\\data\\01_Reference\\20250804_2_nqpd1k_boston_st_gd\\boston_nqpf1k_allPOIS_stgd.shp\"\n",
    "\n",
    "# Step 1: Copy shapefiles to GDB as feature classes\n",
    "shp_files = [f for f in os.listdir(input_shp_folder) if f.endswith(\".shp\")]\n",
    "\n",
    "for shp in shp_files:\n",
    "    shp_path = os.path.join(input_shp_folder, shp)\n",
    "    out_name = os.path.splitext(shp)[0]\n",
    "    arcpy.FeatureClassToFeatureClass_conversion(shp_path, gdb_path, out_name)\n",
    "    print(f\"Copied {shp} to {out_name}\")\n",
    "\n",
    "arcpy.env.workspace = gdb_path\n",
    "\n",
    "# Step 2: Rename NQPD fields uniquely in each POI feature class\n",
    "sdna_fcs = arcpy.ListFeatureClasses(\"boston_sDNA1k_p_*\")\n",
    "renamed_fcs = []\n",
    "\n",
    "for fc in sdna_fcs:\n",
    "    suffix = fc.split(\"boston_sDNA1k_p_\")[-1][:4]\n",
    "    new_field_name = f\"nqpd1k{suffix}\"\n",
    "\n",
    "    fields = arcpy.ListFields(fc)\n",
    "    nqpd_field = next((f.name for f in fields if f.name.upper().startswith(\"NQPD\")), None)\n",
    "    if nqpd_field:\n",
    "        arcpy.AlterField_management(fc, nqpd_field, new_field_name=new_field_name)\n",
    "        print(f\"Renamed field {nqpd_field} to {new_field_name} in {fc}\")\n",
    "        renamed_fcs.append((fc, new_field_name))\n",
    "    else:\n",
    "        print(f\"No NQPD field found in {fc}\")\n",
    "\n",
    "# Step 3: Copy base network for join target\n",
    "base_fc = sdna_fcs[0]  # or your preferred network FC\n",
    "network_fc = \"network_base\"\n",
    "arcpy.CopyFeatures_management(base_fc, network_fc)\n",
    "\n",
    "# Step 4: For each POI FC, spatial join to network, output separate join FCs\n",
    "joined_fcs = []\n",
    "\n",
    "for fc, nqpd_field in renamed_fcs:\n",
    "    out_join_fc = f\"{fc}_joined\"\n",
    "    arcpy.analysis.SpatialJoin(\n",
    "        target_features=network_fc,\n",
    "        join_features=fc,\n",
    "        out_feature_class=out_join_fc,\n",
    "        join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "        join_type=\"KEEP_ALL\",\n",
    "        match_option=\"INTERSECT\"\n",
    "    )\n",
    "    print(f\"Joined {fc} â†’ {out_join_fc}\")\n",
    "    joined_fcs.append((out_join_fc, nqpd_field))\n",
    "\n",
    "# Step 5: Iteratively join each POI join FC back to network by unique ID (assumes 'OBJECTID' or similar unique key)\n",
    "# We'll do attribute joins here to add each NQPD field to network_fc\n",
    "\n",
    "# Use a unique ID field present in all feature classes\n",
    "unique_id_field = \"OBJECTID\"  # Change if your ID field differs\n",
    "\n",
    "for join_fc, nqpd_field in joined_fcs:\n",
    "    temp_joined = \"network_with_\" + nqpd_field\n",
    "    arcpy.JoinField_management(\n",
    "        in_data=network_fc,\n",
    "        in_field=unique_id_field,\n",
    "        join_table=join_fc,\n",
    "        join_field=unique_id_field,\n",
    "        fields=[nqpd_field]\n",
    "    )\n",
    "    print(f\"Joined field {nqpd_field} from {join_fc} into {network_fc}\")\n",
    "\n",
    "# Step 6: Spatial join POP data with field mapping for just POP20 and existing fields\n",
    "field_mappings = arcpy.FieldMappings()\n",
    "\n",
    "# Add all fields from network_fc except shape (you can customize this list)\n",
    "for field in arcpy.ListFields(network_fc):\n",
    "    if field.type != 'Geometry':\n",
    "        fmap = arcpy.FieldMap()\n",
    "        fmap.addInputField(network_fc, field.name)\n",
    "        fmap.outputField.name = field.name\n",
    "        fmap.outputField.aliasName = field.aliasName\n",
    "        field_mappings.addFieldMap(fmap)\n",
    "\n",
    "# Add POP20 field from pop_layer\n",
    "fmap_pop = arcpy.FieldMap()\n",
    "fmap_pop.addInputField(pop_layer, 'POP20')\n",
    "fmap_pop.outputField.name = 'POP20'\n",
    "fmap_pop.outputField.aliasName = 'POP20'\n",
    "field_mappings.addFieldMap(fmap_pop)\n",
    "\n",
    "# Perform spatial join with POP\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=network_fc,\n",
    "    join_features=pop_layer,\n",
    "    out_feature_class=output_final,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_ALL\",\n",
    "    match_option=\"WITHIN_A_DISTANCE\",\n",
    "    search_radius=\"30 Meters\",\n",
    "    field_mapping=field_mappings\n",
    ")\n",
    "print(f\"Final spatial join with POP done: {output_final}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFBfYpfvzqgB"
   },
   "source": [
    "#### Step 5: Export as txt file(s) for IBM SPSS correlation calculations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
